{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4966565,"sourceType":"datasetVersion","datasetId":2880535},{"sourceId":6146260,"sourceType":"datasetVersion","datasetId":3521629},{"sourceId":4298,"sourceType":"modelInstanceVersion","modelInstanceId":3093,"modelId":735}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment 7 | GenAI Internship\n---\n\n## What is a Retrieval Augmented Generation (RAG) system?\n\nLarge Language Models (LLMs) has proven their ability to understand context and provide accurate answers to various NLP tasks, including summarization, Q&A, when prompted. While being able to provide very good answers to questions about information that they were trained with, they tend to hallucinate when the topic is about information that they do \"not know\", i.e. was not included in their training data. Retrieval Augmented Generation combines external resources with LLMs. The main two components of a RAG are therefore a retriever and a generator.  \n \nThe retriever part can be described as a system that is able to encode our data so that can be easily retrieved the relevant parts of it upon queriying it. The encoding is done using text embeddings, i.e. a model trained to create a vector representation of the information. The best option for implementing a retriever is a vector database. As vector database, there are multiple options, both open source or commercial products. Few examples are ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Our option in this Notebook will be a local instance of ChromaDB (persistent).\n\nFor the generator part, the obvious option is a LLM. In this Notebook\n - We will use a quantized LLaMA v2 model, from the Kaggle Models collection.  \n - We will use a \"Enter the other model(s) used\", from the Kaggle Models collection.\n\nThe orchestration of the retriever and generator will be done using Langchain. A specialized function from Langchain allows us to create the receiver-generator in one line of code.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T04:42:06.430632Z","iopub.execute_input":"2024-08-03T04:42:06.431461Z","iopub.status.idle":"2024-08-03T04:42:06.442325Z","shell.execute_reply.started":"2024-08-03T04:42:06.431401Z","shell.execute_reply":"2024-08-03T04:42:06.441353Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Installing and Importing Libraries and Utilities","metadata":{}},{"cell_type":"code","source":"!pip install \\\ntransformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\nbitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12 pyarrow","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-03T04:42:06.444563Z","iopub.execute_input":"2024-08-03T04:42:06.444896Z","iopub.status.idle":"2024-08-03T04:45:00.007317Z","shell.execute_reply.started":"2024-08-03T04:42:06.444865Z","shell.execute_reply":"2024-08-03T04:45:00.006002Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.33.0 in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: accelerate==0.22.0 in /opt/conda/lib/python3.10/site-packages (0.22.0)\nCollecting einops==0.6.1\n  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting langchain==0.0.300\n  Downloading langchain-0.0.300-py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting xformers==0.0.21\n  Downloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl (167.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting bitsandbytes==0.41.1\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting sentence_transformers==2.2.2\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting chromadb==0.4.12\n  Downloading chromadb-0.4.12-py3-none-any.whl (426 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.5/426.5 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (11.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (2.0.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (2.0.17)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (3.8.4)\nRequirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (3.7.0)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (0.6.0)\nCollecting jsonpatch<2.0,>=1.33 (from langchain==0.0.300)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nCollecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.300)\n  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (2.8.5)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (1.10.9)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (8.2.2)\nCollecting torch>=1.10.0 (from accelerate==0.22.0)\n  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.15.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.11.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.1.99)\nCollecting chroma-hnswlib==0.7.3 (from chromadb==0.4.12)\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.98.0)\nRequirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.22.0)\nCollecting posthog>=2.4.0 (from chromadb==0.4.12)\n  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (4.6.3)\nCollecting pulsar-client>=3.1.0 (from chromadb==0.4.12)\n  Downloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.4.12)\n  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pypika>=0.48.9 (from chromadb==0.4.12)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting overrides>=7.3.1 (from chromadb==0.4.12)\n  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (5.12.0)\nCollecting bcrypt>=4.0.1 (from chromadb==0.4.12)\n  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.2)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==2.0.0 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (68.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (0.40.0)\nCollecting cmake (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0)\n  Downloading cmake-3.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lit (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0)\n  Downloading lit-18.1.8-py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.1.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (3.20.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (0.9.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.12) (0.27.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2023.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.300) (2.0)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.12)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (3.20.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.33.0) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.16.0)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.12)\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.2.1)\nRequirement already satisfied: python-dateutil>2.1 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.8.2)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.12) (2023.7.22)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0) (1.26.15)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.300) (2.0.2)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.6.0)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.17.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.20.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (11.0.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers==2.2.2) (9.5.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (1.0.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.12)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\nBuilding wheels for collected packages: sentence_transformers, pypika\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=ccacc197d7900aacfff91f957d737bcdbd1ab1a326b300e73653f392073716c3\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=87efddaddbca8116cac45a3e8bd06061f12ae19d6a24c1f3193ba856dedf41bd\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built sentence_transformers pypika\nInstalling collected packages: pypika, monotonic, lit, bitsandbytes, pulsar-client, overrides, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, jsonpatch, humanfriendly, einops, cmake, chroma-hnswlib, bcrypt, posthog, nvidia-cusolver-cu11, nvidia-cudnn-cu11, langsmith, coloredlogs, onnxruntime, langchain, chromadb, triton, torch, xformers, sentence_transformers\n  Attempting uninstall: overrides\n    Found existing installation: overrides 6.5.0\n    Uninstalling overrides-6.5.0:\n      Successfully uninstalled overrides-6.5.0\n  Attempting uninstall: jsonpatch\n    Found existing installation: jsonpatch 1.32\n    Uninstalling jsonpatch-1.32:\n      Successfully uninstalled jsonpatch-1.32\n  Attempting uninstall: torch\n    Found existing installation: torch 2.0.0\n    Uninstalling torch-2.0.0:\n      Successfully uninstalled torch-2.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.7.0 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bcrypt-4.2.0 bitsandbytes-0.41.1 chroma-hnswlib-0.7.3 chromadb-0.4.12 cmake-3.30.1 coloredlogs-15.0.1 einops-0.6.1 humanfriendly-10.0 jsonpatch-1.33 langchain-0.0.300 langsmith-0.0.92 lit-18.1.8 monotonic-1.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 onnxruntime-1.18.1 overrides-7.3.1 posthog-3.5.0 pulsar-client-3.5.0 pypika-0.48.9 sentence_transformers-2.2.2 torch-2.0.1 triton-2.0.0 xformers-0.0.21\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nimport transformers\nimport chromadb\nimport pandas as pd\n\nfrom time import time\nfrom torch import cuda, bfloat16\nfrom transformers import AutoTokenizer\nfrom chromadb.config import Settings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:45:00.008749Z","iopub.execute_input":"2024-08-03T04:45:00.009042Z","iopub.status.idle":"2024-08-03T04:45:07.570582Z","shell.execute_reply.started":"2024-08-03T04:45:00.009015Z","shell.execute_reply":"2024-08-03T04:45:07.569582Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Initializing Model, Tokenizer and Setting up Query Pipeline","metadata":{}},{"cell_type":"markdown","source":"Define the model, the device, and the `bitsandbytes` configuration.","metadata":{}},{"cell_type":"markdown","source":"## Creating a model from Meta llama 2","metadata":{}},{"cell_type":"code","source":"model_llama2 = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:45:07.571950Z","iopub.execute_input":"2024-08-03T04:45:07.572435Z","iopub.status.idle":"2024-08-03T04:45:07.631500Z","shell.execute_reply.started":"2024-08-03T04:45:07.572401Z","shell.execute_reply":"2024-08-03T04:45:07.630716Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Prepare the model and the tokenizer.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_llama2,\n)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_llama2,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\ntokenizer = AutoTokenizer.from_pretrained(model_llama2)\ntime_2 = time()\nprint(f\"Preparing Model and Tokenizer took : {round(time_2-time_1, 3)} second(s)\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:45:07.633798Z","iopub.execute_input":"2024-08-03T04:45:07.634083Z","iopub.status.idle":"2024-08-03T04:48:08.886295Z","shell.execute_reply.started":"2024-08-03T04:45:07.634058Z","shell.execute_reply":"2024-08-03T04:48:08.885284Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b576cbb3d530494f9df4b732e68a0ed0"}},"metadata":{}},{"name":"stdout","text":"Preparing Model and Tokenizer took : 181.247 second(s)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Define the query pipeline.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\nquery_pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\ntime_2 = time()\nprint(f\"Preparing the Pipeline took : {round(time_2-time_1, 3)} second(s)\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:48:08.887444Z","iopub.execute_input":"2024-08-03T04:48:08.887794Z","iopub.status.idle":"2024-08-03T04:48:10.798907Z","shell.execute_reply.started":"2024-08-03T04:48:08.887759Z","shell.execute_reply":"2024-08-03T04:48:10.797921Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Preparing the Pipeline took : 1.906 second(s)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### We define a function for testing the pipeline.","metadata":{}},{"cell_type":"code","source":"def test_model(tokenizer, pipeline, prompt_to_test):\n    \"\"\"\n    Perform a query\n    print the result\n    Args:\n        tokenizer: the tokenizer\n        pipeline: the pipeline\n        prompt_to_test: the prompt\n    Returns\n        None\n    \"\"\"\n    # adapted from https://huggingface.co/blog/llama2#using-transformers\n    time_1 = time()\n    sequences = pipeline(\n        prompt_to_test,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,)\n    time_2 = time()\n    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:48:10.800126Z","iopub.execute_input":"2024-08-03T04:48:10.800431Z","iopub.status.idle":"2024-08-03T04:48:10.806900Z","shell.execute_reply.started":"2024-08-03T04:48:10.800397Z","shell.execute_reply":"2024-08-03T04:48:10.805927Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Testing the Query Pipeline\n\nWe test the pipeline with a query about random topics.","metadata":{}},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Please explain about Marvel Cinematic Universe Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:48:10.808696Z","iopub.execute_input":"2024-08-03T04:48:10.809061Z","iopub.status.idle":"2024-08-03T04:48:22.609557Z","shell.execute_reply.started":"2024-08-03T04:48:10.809035Z","shell.execute_reply":"2024-08-03T04:48:22.608414Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Test inference: 11.781 sec.\nResult: Please explain about Marvel Cinematic Universe Give just a definition. Keep it in 100 words.\nThe Marvel Cinematic Universe (MCU) is a franchise of interconnected superhero films, television shows, and other media produced by Marvel Studios. The franchise began with Iron Man (2008) and has since grown to include 22 films, several television shows, and numerous characters, including Iron Man, Captain America, Thor, Black Widow, the Avengers, and many others. The MCU is known for its complex, interconnected storytelling and its ability to bring iconic Marvel Comics characters to life on the big screen.\n","output_type":"stream"}]},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Please explain Computers, Keep it in 100 words.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T04:48:22.612856Z","iopub.execute_input":"2024-08-03T04:48:22.613184Z","iopub.status.idle":"2024-08-03T04:48:33.496873Z","shell.execute_reply.started":"2024-08-03T04:48:22.613148Z","shell.execute_reply":"2024-08-03T04:48:33.495931Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Test inference: 10.879 sec.\nResult: Please explain Computers, Keep it in 100 words. Unterscheidung Between Computer System, Network And Internet.\nA computer system refers to the hardware and software components of a computer, such as the processor, memory, storage devices, and input/output devices, that work together to perform computations and process data. A network is a collection of interconnected devices, such as computers, servers, and routers, that communicate with each other over a shared communication medium, such as a LAN or the internet. The internet is a global network of interconnected devices, such as computers, servers, and routers, that communicate with each other over a shared communication medium, such as the internet.\n","output_type":"stream"}]},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Tell me activities to do while in India.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T04:48:33.498958Z","iopub.execute_input":"2024-08-03T04:48:33.499707Z","iopub.status.idle":"2024-08-03T04:48:49.411840Z","shell.execute_reply.started":"2024-08-03T04:48:33.499658Z","shell.execute_reply":"2024-08-03T04:48:49.410923Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Test inference: 15.908 sec.\nResult: Tell me activities to do while in India. Unterscheidung: between 'while' and 'whilst'while 'while' is a conjunction used to connect a main clause with an adverbial clause,'whiles'is a noun phrase used to refer to a period of time.\nThe country of India is home to a diverse range of cultures, traditions, and landscapes, making it a fascinating destination for any traveler. Here are some activities to do while you're in India: 1. Explore the Taj Mahal - One of the most famous historical landmarks in India, the Taj Mahal is a must-visit attraction. Visit the white marble mausoleum in Agra, Uttar Pradesh, and take in its stunning beauty. 2. Take a boat ride on Lake Pichola - Located in Udaipur, Lake Pichola is a beautiful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Retrieval Augmented Generation","metadata":{}},{"cell_type":"markdown","source":"## Check the model with a HuggingFace pipeline\n\n\nWe check the model with a HF pipeline, using a query about the same 3 random topics.","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:22:16.433666Z","iopub.execute_input":"2023-09-23T19:22:16.434937Z","iopub.status.idle":"2023-09-23T19:22:16.440864Z","shell.execute_reply.started":"2023-09-23T19:22:16.434891Z","shell.execute_reply":"2023-09-23T19:22:16.439217Z"}}},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=query_pipeline)\n# checking again that everything is working fine\nllm(prompt=\"Please explain about Marvel Cinematic Universe Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:48:49.412935Z","iopub.execute_input":"2024-08-03T04:48:49.413231Z","iopub.status.idle":"2024-08-03T04:48:58.119938Z","shell.execute_reply.started":"2024-08-03T04:48:49.413206Z","shell.execute_reply":"2024-08-03T04:48:58.119049Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'\\nThe Marvel Cinematic Universe (MCU) is a series of interconnected superhero films produced by Marvel Studios, based on characters from Marvel Comics. The MCU includes 23 films, starting with Iron Man in 2008 and most recently including Avengers: Endgame in 2019. The franchise has become a cultural phenomenon, connecting various superheroes and stories across different films, and has grossed over $22 billion worldwide.'"},"metadata":{}}]},{"cell_type":"code","source":"llm(prompt=\"Please explain Computers, Keep it in 100 words.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T04:48:58.121070Z","iopub.execute_input":"2024-08-03T04:48:58.121370Z","iopub.status.idle":"2024-08-03T04:49:05.826474Z","shell.execute_reply.started":"2024-08-03T04:48:58.121344Z","shell.execute_reply":"2024-08-03T04:49:05.825573Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"' Unterscheidung zwischen Computer und Computer system. A computer system consists of several components that work together to perform various tasks. A computer system includes a central processing unit (CPU), memory, input/output devices, and storage devices. The CPU performs calculations and executes instructions, while memory stores data and programs. Input/output devices allow users to interact with the computer, and storage devices provide long-term storage for data and programs. (Source: Wikipedia)'"},"metadata":{}}]},{"cell_type":"code","source":"llm(prompt=\"Tell me activities to do while in India.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T04:49:05.827731Z","iopub.execute_input":"2024-08-03T04:49:05.828097Z","iopub.status.idle":"2024-08-03T04:49:49.844259Z","shell.execute_reply.started":"2024-08-03T04:49:05.828061Z","shell.execute_reply":"2024-08-03T04:49:49.843080Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"' Unterscheidung between the two is not always clear-cut, and the terms are often used interchangeably. The country has a diverse landscape, with mountains, deserts, forests, and coastlines, offering a wide range of outdoor activities. India is a vast and diverse country, and there are many exciting activities to do while visiting. Here are some of the best things to do in India: 1. Visit the Taj Mahal: The Taj Mahal is one of the most iconic landmarks in India and a must-visit attraction for anyone traveling to the country. 2. Explore the Himalayas: The Himalayas offer some of the most beautiful and challenging treks in the world, including the famous Kailash Manasarovar Yatra. 3. Go on a wildlife safari: India is home to a wide variety of wildlife, including the majestic Bengal tiger, and there are many national parks and sanctuaries where you can go on a wildlife safari. 4. Visit the beaches of Goa: Goa is famous for its beautiful beaches, including Palolem, Vagator, and Anjuna, which offer a range of water sports and activities. 5. Take a boat ride on the Ganges: A boat ride on the Ganges River is a unique and memorable experience, and a great way to see the city of Varanasi from a different perspective. 6. Learn about Indian culture: India has a rich and diverse culture, and there are many opportunities to learn about it, including visiting museums, attending cultural events, and taking cooking classes. 7. Go on a food tour: Indian cuisine is famous for its diversity and complexity, and a food tour is a great way to experience the local flavors and culinary delights. 8. Visit the temples of South India: South India is home to many beautiful temples, including the famous Meenakshi Amman Temple in Madurai and the Kashi Vishwanath Temple in Varanasi. 9. Take a yoga class: India is the birthplace of yoga, and there are many yoga studios and retreats where you can take a class and learn about the ancient practice. 10. Go on a road trip: India has a vast and diverse landscape, and a road trip is a great way to see the country and experience its many different landscapes and cultures.\\n\\n\\n\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Ingestion of data using Text loder","metadata":{}},{"cell_type":"markdown","source":"#### Using WikiPedia Data(01-07-2023) as our text data. This data is originally present in a-z indexed parquet files(.parquet), we change it to text files(.txt).","metadata":{}},{"cell_type":"code","source":"# Specify the directory containing the Parquet files\nparquet_dir = '/kaggle/input/wikipedia-20230701'\ntxt_dir = '/kaggle/working/'\n\n# Define the number of examples (rows) to load from each Parquet file\nnum_examples = 1200  # Adjust this number based on your memory constraints\n\n# Create the TXT directory if it doesn't exist\nif not os.path.exists(txt_dir):\n    os.makedirs(txt_dir)\n\n# Loop through all the Parquet files in the directory\nfor filename in os.listdir(parquet_dir):\n    if filename.endswith(\".parquet\"):\n        # Read only a subset of the Parquet file into a DataFrame\n        df = pd.read_parquet(os.path.join(parquet_dir, filename), engine='pyarrow')\n        df_subset = df.head(num_examples)  # Adjust this to select different subsets, e.g., df.sample(num_examples)\n\n        # Specify the output TXT file name\n        txt_filename = os.path.splitext(filename)[0] + '.txt'\n        txt_filepath = os.path.join(txt_dir, txt_filename)\n\n        # Write DataFrame subset to a TXT file (default separator is tab)\n        df_subset.to_csv(txt_filepath, sep='\\t', index=False, header=True)\n        print(f\"Converted {filename} to {txt_filename} with {num_examples} examples\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T04:49:49.849595Z","iopub.execute_input":"2024-08-03T04:49:49.849905Z","iopub.status.idle":"2024-08-03T04:55:44.877911Z","shell.execute_reply.started":"2024-08-03T04:49:49.849878Z","shell.execute_reply":"2024-08-03T04:55:44.876908Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Converted x.parquet to x.txt with 1200 examples\nConverted h.parquet to h.txt with 1200 examples\nConverted w.parquet to w.txt with 1200 examples\nConverted g.parquet to g.txt with 1200 examples\nConverted a.parquet to a.txt with 1200 examples\nConverted y.parquet to y.txt with 1200 examples\nConverted l.parquet to l.txt with 1200 examples\nConverted n.parquet to n.txt with 1200 examples\nConverted i.parquet to i.txt with 1200 examples\nConverted number.parquet to number.txt with 1200 examples\nConverted j.parquet to j.txt with 1200 examples\nConverted m.parquet to m.txt with 1200 examples\nConverted b.parquet to b.txt with 1200 examples\nConverted r.parquet to r.txt with 1200 examples\nConverted v.parquet to v.txt with 1200 examples\nConverted z.parquet to z.txt with 1200 examples\nConverted o.parquet to o.txt with 1200 examples\nConverted wiki_2023_index.parquet to wiki_2023_index.txt with 1200 examples\nConverted k.parquet to k.txt with 1200 examples\nConverted q.parquet to q.txt with 1200 examples\nConverted e.parquet to e.txt with 1200 examples\nConverted f.parquet to f.txt with 1200 examples\nConverted p.parquet to p.txt with 1200 examples\nConverted t.parquet to t.txt with 1200 examples\nConverted other.parquet to other.txt with 1200 examples\nConverted d.parquet to d.txt with 1200 examples\nConverted u.parquet to u.txt with 1200 examples\nConverted s.parquet to s.txt with 1200 examples\nConverted c.parquet to c.txt with 1200 examples\n","output_type":"stream"}]},{"cell_type":"code","source":"# List to hold all the documents\nall_documents = []\n\n# Loop through all text files in the directory\nfor filename in os.listdir(txt_dir):\n    if filename.endswith(\".txt\"):\n        # Full path to the text file\n        file_path = os.path.join(txt_dir, filename)\n        \n        # Load the text file\n        loader = TextLoader(file_path, encoding=\"utf8\")\n        documents = loader.load()\n        \n        # Append loaded documents to the list\n        all_documents.extend(documents)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T04:55:44.879441Z","iopub.execute_input":"2024-08-03T04:55:44.880276Z","iopub.status.idle":"2024-08-03T04:55:45.944019Z","shell.execute_reply.started":"2024-08-03T04:55:44.880231Z","shell.execute_reply":"2024-08-03T04:55:45.943143Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Split data in chunks\n\nWe split data in chunks using a recursive character text splitter.","metadata":{}},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:55:45.945258Z","iopub.execute_input":"2024-08-03T04:55:45.946049Z","iopub.status.idle":"2024-08-03T04:55:47.810910Z","shell.execute_reply.started":"2024-08-03T04:55:45.946014Z","shell.execute_reply":"2024-08-03T04:55:47.810116Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Creating Embeddings and Storing in Vector Store","metadata":{}},{"cell_type":"markdown","source":"Create the embeddings using Sentence Transformer and HuggingFace embeddings.","metadata":{}},{"cell_type":"code","source":"model_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:55:47.812387Z","iopub.execute_input":"2024-08-03T04:55:47.812801Z","iopub.status.idle":"2024-08-03T04:55:55.474509Z","shell.execute_reply.started":"2024-08-03T04:55:47.812765Z","shell.execute_reply":"2024-08-03T04:55:55.473632Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading .gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3758d9b9b5e4b329b31866569098bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de34539e88a84ae48235ce6573bd89ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8bf6b0b89b44a2081488b5669e22734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4e84fd9c3c44732b5ffa1a26dd31649"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c081e433e4af4e9b85253869571e6a30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b82066e74f144cc89c4339d2eb59307c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50727aba796f4aab9a6d0ed06062c4f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02319eb3cae44435bf925063edaa1f7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12e7e1e6be654525b18bc0ff02b66f83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d73df89df4e94d268f601d95f9b63675"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7077c21b19b14c079787bdebfd3a1c88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29c0bf11b16b42b1bee59dc19caa6d3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5571410edbb44ad5a7c48867dc527168"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e5c10cf7fc94886b17dccf94d5c124a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b44655f5d7614ee6b5d66ea00affa2d6"}},"metadata":{}}]},{"cell_type":"markdown","source":"Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally.","metadata":{}},{"cell_type":"code","source":"vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:55:55.475662Z","iopub.execute_input":"2024-08-03T04:55:55.475973Z","iopub.status.idle":"2024-08-03T04:56:58.285504Z","shell.execute_reply.started":"2024-08-03T04:55:55.475945Z","shell.execute_reply":"2024-08-03T04:56:58.284661Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/183 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"950acb6e5c084ebab21a71bab7c04f7e"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Initialize chain","metadata":{}},{"cell_type":"code","source":"retriever = vectordb.as_retriever()\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:56:58.287045Z","iopub.execute_input":"2024-08-03T04:56:58.287355Z","iopub.status.idle":"2024-08-03T04:56:58.292571Z","shell.execute_reply.started":"2024-08-03T04:56:58.287328Z","shell.execute_reply":"2024-08-03T04:56:58.291610Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Test the Retrieval-Augmented Generation \n\n\nWe define a test function, that will run the query and time it.","metadata":{}},{"cell_type":"code","source":"def test_rag(qa, query):\n    print(f\"Query: {query}\\n\")\n    time_1 = time()\n    result = qa.run(query)\n    time_2 = time()\n    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n    print(\"\\nResult: \", result)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:56:58.293730Z","iopub.execute_input":"2024-08-03T04:56:58.294075Z","iopub.status.idle":"2024-08-03T04:56:58.306812Z","shell.execute_reply.started":"2024-08-03T04:56:58.294050Z","shell.execute_reply":"2024-08-03T04:56:58.305761Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Let's check few queries.","metadata":{}},{"cell_type":"code","source":"query = \"Please explain about Marvel Cinematic Universe Give just a definition. Keep it in 100 words.\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:56:58.307886Z","iopub.execute_input":"2024-08-03T04:56:58.308187Z","iopub.status.idle":"2024-08-03T04:57:09.188534Z","shell.execute_reply.started":"2024-08-03T04:56:58.308155Z","shell.execute_reply":"2024-08-03T04:57:09.187630Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Query: Please explain about Marvel Cinematic Universe Give just a definition. Keep it in 100 words.\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d52e919759f14faabf3287279a4cb2dd"}},"metadata":{}},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 10.87 sec.\n\nResult:   The Marvel Cinematic Universe (MCU) is a series of interconnected superhero films produced by Marvel Studios, based on characters from the Marvel Comics universe. The franchise began with Iron Man (2008) and has since grown to include 23 films, with many more in development. The MCU is known for its complex, interconnected storylines and its use of shared universe elements, such as crossover events and cameos from beloved characters.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Please explain Computers, Keep it in 100 words.\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:57:09.189891Z","iopub.execute_input":"2024-08-03T04:57:09.190219Z","iopub.status.idle":"2024-08-03T04:57:20.925218Z","shell.execute_reply.started":"2024-08-03T04:57:09.190184Z","shell.execute_reply":"2024-08-03T04:57:20.924273Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Query: Please explain Computers, Keep it in 100 words.\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef413f3680d049c3a51d4127980c38f3"}},"metadata":{}},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 11.731 sec.\n\nResult:   Ali is a Bangladeshi origin-Australian computer scientist and data analyst. He is the author of several books in the area of Data Mining, Computational Intelligence, and Smart Grid. He is a newspaper columnist, academic, and well-known researcher in the areas of Machine Learning and Data Science. He is the founder of a research center and international conferences in Data Science and Engineering. He served widely in the international community and is a well-known international keynote speaker.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Tell me activities to do while in India.\"\ntest_rag(qa, query)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T04:57:20.926535Z","iopub.execute_input":"2024-08-03T04:57:20.926848Z","iopub.status.idle":"2024-08-03T04:57:38.897086Z","shell.execute_reply.started":"2024-08-03T04:57:20.926824Z","shell.execute_reply":"2024-08-03T04:57:38.896177Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Query: Tell me activities to do while in India.\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"358a87361003426bbabc15f7a1fae5b0"}},"metadata":{}},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 17.966 sec.\n\nResult:   There are many fun and interesting activities to do while in India. Some popular options include visiting historical sites and landmarks, such as the Taj Mahal or the Red Fort in Delhi. You could also take a boat ride on the Ganges River in Varanasi, or explore the vibrant markets and bazaars of cities like Mumbai or Jaipur. If you're looking for something more adventurous, you could try white water rafting in the Himalayas or go on a wildlife safari in one of India's many national parks. Whatever you choose, you're sure to have a memorable and exciting experience in India!\n\nUnhelpful Answer: I don't know, I'm just an AI and I don't have personal experiences or knowledge of India. I can't provide you with any activities to do while in India.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Document sources\n\nLet's check the documents sources, for the last query run.","metadata":{}},{"cell_type":"code","source":"docs = vectordb.similarity_search(query)\nprint(f\"Query: {query}\")\nprint(f\"Retrieved documents: {len(docs)}\")\nfor doc in docs:\n    doc_details = doc.to_json()['kwargs']\n    print(\"Source: \", doc_details['metadata']['source'])\n    print(\"Text: \", doc_details['page_content'], \"\\n\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-08-03T04:57:38.898322Z","iopub.execute_input":"2024-08-03T04:57:38.898626Z","iopub.status.idle":"2024-08-03T04:57:38.939326Z","shell.execute_reply.started":"2024-08-03T04:57:38.898599Z","shell.execute_reply":"2024-08-03T04:57:38.938398Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d1327384b144793b20b58d89285c96f"}},"metadata":{}},{"name":"stdout","text":"Query: Tell me activities to do while in India.\nRetrieved documents: 4\nSource:  /kaggle/working/a.txt\nText:  Shankar in first post-attack Mumbai concert Category:Benefit concerts Category:Terrorism in India Category:2008 in music\"\t['Benefit concerts' 'Terrorism in India' '2008 in music'] \n\nSource:  /kaggle/working/a.txt\nText:  Film Award for Best Male Actor – Upendra * Udaya Film Award for Best Music Director – Gurukiran * Karnataka State Film Award for Best Sound Recording – Murali Rayasam * Karnataka State Film Award for Best Editor – T. Shashikumar ==References== ==External links== * Category:Films set in Bangalore Category:1998 films Category:1990s Kannada- language films Category:1990s psychological thriller films Category:Films about filmmaking Category:Films scored by Gurukiran Category:Kannada films remade in other languages Category:Indian nonlinear narrative films Category:Films directed by Upendra Category:Indian psychological thriller films\"\t\"['Films set in Bangalore' '1998 films' '1990s Kannada- language films' \n\nSource:  /kaggle/working/a.txt\nText:  that abound in contemporary narratives on India.\"\" Amrita Dutta writes in The Indian Express, \"\"Majumdar's economy of style and language extends, unfortunately, to an economy of specificities and details – one that produces an attenuated version of the complex, violent remaking of India, a version that is evidently easier on Western critics and publishing cultures.\"\" Rihan Najib writes in The Hindu Business Line, \"\"Though the novel is adept at essaying contemporary Indian realities — taking on an Arundhati Roy- esque array of social justice issues such as development-induced displacement, Islamophobia, media trials, transphobia, income inequality, cow vigilantism — it does so with the broadest possible brush. [...] Nevertheless, Majumdar’s talents as a writer of the times is undeniable and she is certain to reach higher echelons of literary fame.\"\" Political anthropologist Irfan Ahmad wrote an extensive critique of the novel in The Caravan, stating, \"\"What the novel does best is make \n\nSource:  /kaggle/working/a.txt\nText:  engagement party and confesses his love, saying that he will keep her happy in any way she wants. Mahalakshmi demands Anand to let go of her daughter, but Anand confronts her for how she cheated his father out of the money she owed him. In the end, he leaves with Anasuya. When Anasuya and Anand reach his house, she is surprised to see her parents there. Her father reveals that as soon as Anasuya and Anand left, Mahalakshmi had a panic attack, but he managed to calm her down and get through to her about why they should let them be together. In the end, both families make amends for their past mistakes. Shekar also visits soon after to ask for Bhanu's hand in marriage. And lastly, Nagavalli seems to be coping with the fact that Anand isn't going to marry her...but she then says to her father that if the pair were to separate one day, Anand would return to her. == Cast == * Nithiin as Anand \"\"Nandu\"\" Vihari * Samantha Ruth Prabhu as Anasuya \"\"Anu\"\" Ramalingam * Ananya as Bhanumathi \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### References\n- Dataset : https://www.kaggle.com/datasets/jjinho/wikipedia-20230701\n- Original Reference Notebook : https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb","metadata":{}}]}