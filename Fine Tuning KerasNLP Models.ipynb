{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6082,"sourceType":"modelInstanceVersion","modelInstanceId":4697,"modelId":2823},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171,"modelId":3533},{"sourceId":27825,"sourceType":"modelInstanceVersion","modelInstanceId":22009,"modelId":3533}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1>Assignment 6 : Fine-Tuning KerasNLP Models using Low Rank Adaptation (LoRA)\n\n---","metadata":{}},{"cell_type":"markdown","source":"**Please Select Accelerator as GPU T4 x2**","metadata":{}},{"cell_type":"markdown","source":"<href>Google Gemma Documentations : https://ai.google.dev/gemma/docs/lora_tuning<br>\n<href>KerasNLP Models : https://keras.io/api/keras_nlp/models/","metadata":{"execution":{"iopub.status.busy":"2024-07-20T04:55:05.102611Z","iopub.execute_input":"2024-07-20T04:55:05.103322Z","iopub.status.idle":"2024-07-20T04:55:05.115935Z","shell.execute_reply.started":"2024-07-20T04:55:05.103285Z","shell.execute_reply":"2024-07-20T04:55:05.114673Z"}}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:08:35.295056Z","iopub.execute_input":"2024-07-21T10:08:35.295420Z","iopub.status.idle":"2024-07-21T10:08:35.309136Z","shell.execute_reply.started":"2024-07-21T10:08:35.295377Z","shell.execute_reply":"2024-07-21T10:08:35.308166Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install -q -U keras-nlp\n!pip install -q -U keras","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:08:35.524362Z","iopub.execute_input":"2024-07-21T10:08:35.525126Z","iopub.status.idle":"2024-07-21T10:09:14.432722Z","shell.execute_reply.started":"2024-07-21T10:08:35.525087Z","shell.execute_reply":"2024-07-21T10:09:14.431703Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.2)\nCollecting pip\n  Downloading pip-24.1.2-py3-none-any.whl.metadata (3.6 kB)\nDownloading pip-24.1.2-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.3.2\n    Uninstalling pip-23.3.2:\n      Successfully uninstalled pip-23.3.2\nSuccessfully installed pip-24.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Importing Required Dependancies/Libraries\n\nimport os\nimport keras_nlp\nimport keras\nimport json\nimport kagglehub\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:09:14.434982Z","iopub.execute_input":"2024-07-21T10:09:14.435345Z","iopub.status.idle":"2024-07-21T10:09:27.253148Z","shell.execute_reply.started":"2024-07-21T10:09:14.435311Z","shell.execute_reply":"2024-07-21T10:09:27.252264Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-07-21 10:09:16.147669: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-21 10:09:16.147802: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-21 10:09:16.272639: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ[\"KERAS_NLP_API\"] = \"jax\" # or \"torch\" or \"tensorflow\"\n# Avoid memory fragmentation\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:09:27.254235Z","iopub.execute_input":"2024-07-21T10:09:27.254774Z","iopub.status.idle":"2024-07-21T10:09:27.259240Z","shell.execute_reply.started":"2024-07-21T10:09:27.254746Z","shell.execute_reply":"2024-07-21T10:09:27.258266Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Retrieving the Dataset\n!wget -O databricks-dolly-15k.jsonl https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:09:27.261645Z","iopub.execute_input":"2024-07-21T10:09:27.262088Z","iopub.status.idle":"2024-07-21T10:09:28.915162Z","shell.execute_reply.started":"2024-07-21T10:09:27.262052Z","shell.execute_reply":"2024-07-21T10:09:28.914177Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"--2024-07-21 10:09:28--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\nResolving huggingface.co (huggingface.co)... 18.172.134.4, 18.172.134.124, 18.172.134.88, ...\nConnecting to huggingface.co (huggingface.co)|18.172.134.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1721814505&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMTgxNDUwNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=b5Exrxn8kFHJE0hBno9WMTDI2aio4pNzII5ryy%7EDXm-clFICnS-G%7E1tKNQWTdDVe6JmowbkQ2UWuBklUoAbUjpzyp6tSbjFgkMAk7Sex27L05%7E%7E6R6WAzOaZU3YsV3RlMPE5uf94hnqh0WXEWAE4X7OAhGO9NAgxfyp026q4ihMib5L10Coh-xOi3Ps52QLz%7EpurWuKiwHD2NR%7Ei51Fc1EgTjUuwTtUtzlz5X94%7Ej%7E6WkTSo51OGYG4f9inuBbitwvtTtUxTJ2BzV4MN14eEtF2FEiU9oNh44I04ZNUdtTnOzlaujsovqYcqUmDu%7EnTDh0Jpm8dLfggLCPCMyHhyRQ__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n--2024-07-21 10:09:28--  https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8''databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1721814505&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMTgxNDUwNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=b5Exrxn8kFHJE0hBno9WMTDI2aio4pNzII5ryy~DXm-clFICnS-G~1tKNQWTdDVe6JmowbkQ2UWuBklUoAbUjpzyp6tSbjFgkMAk7Sex27L05~~6R6WAzOaZU3YsV3RlMPE5uf94hnqh0WXEWAE4X7OAhGO9NAgxfyp026q4ihMib5L10Coh-xOi3Ps52QLz~purWuKiwHD2NR~i51Fc1EgTjUuwTtUtzlz5X94~j~6WkTSo51OGYG4f9inuBbitwvtTtUxTJ2BzV4MN14eEtF2FEiU9oNh44I04ZNUdtTnOzlaujsovqYcqUmDu~nTDh0Jpm8dLfggLCPCMyHhyRQ__&Key-Pair-Id=K3ESJI6DHPFC7\nResolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.142.125, 108.157.142.12, 108.157.142.121, ...\nConnecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.142.125|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13085339 (12M) [text/plain]\nSaving to: 'databricks-dolly-15k.jsonl'\n\ndatabricks-dolly-15 100%[===================>]  12.48M  46.3MB/s    in 0.3s    \n\n2024-07-21 10:09:28 (46.3 MB/s) - 'databricks-dolly-15k.jsonl' saved [13085339/13085339]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"data = []\nwith open(\"databricks-dolly-15k.jsonl\") as file:\n    for line in file:\n        features = json.loads(line)\n        # Filter out examples with context, to keep it simple.\n        if features[\"context\"]:\n            continue\n        # Format the entire example as a single string.\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n        data.append(template.format(**features))\n\n# Only use 1000 training examples, to keep it fast.\ndata = data[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:09:28.916617Z","iopub.execute_input":"2024-07-21T10:09:28.916922Z","iopub.status.idle":"2024-07-21T10:09:29.064952Z","shell.execute_reply.started":"2024-07-21T10:09:28.916892Z","shell.execute_reply":"2024-07-21T10:09:29.064167Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"display(data[0:5])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:09:29.065988Z","iopub.execute_input":"2024-07-21T10:09:29.066258Z","iopub.status.idle":"2024-07-21T10:09:29.074058Z","shell.execute_reply.started":"2024-07-21T10:09:29.066234Z","shell.execute_reply":"2024-07-21T10:09:29.073138Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"['Instruction:\\nWhich is a species of fish? Tope or Rope\\n\\nResponse:\\nTope',\n 'Instruction:\\nWhy can camels survive for long without water?\\n\\nResponse:\\nCamels use the fat in their humps to keep them filled with energy and hydration for long periods of time.',\n \"Instruction:\\nAlice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\\n\\nResponse:\\nThe name of the third daughter is Alice\",\n 'Instruction:\\nWho gave the UN the land in NY to build their HQ\\n\\nResponse:\\nJohn D Rockerfeller',\n 'Instruction:\\nWhy mobile is bad for human\\n\\nResponse:\\nWe are always engaged one phone which is not good.']"},"metadata":{}}]},{"cell_type":"markdown","source":"<h1><center>Gemma Model","metadata":{}},{"cell_type":"code","source":"# Creating the Gemma language model\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:09:29.075239Z","iopub.execute_input":"2024-07-21T10:09:29.075583Z","iopub.status.idle":"2024-07-21T10:10:39.502885Z","shell.execute_reply.started":"2024-07-21T10:09:29.075557Z","shell.execute_reply":"2024-07-21T10:10:39.502007Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Attaching 'model.safetensors' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stderr","text":"normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"<h3>Prompting on Default Gemma LLM","metadata":{}},{"cell_type":"code","source":"# Attempting to prompt model to give answer for instruction.\nprompt = template.format(\n    instruction=\"What should I do on a trip to Europe?\",\n    response=\"\",\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:10:39.504226Z","iopub.execute_input":"2024-07-21T10:10:39.504597Z","iopub.status.idle":"2024-07-21T10:11:18.235403Z","shell.execute_reply.started":"2024-07-21T10:10:39.504564Z","shell.execute_reply":"2024-07-21T10:11:18.234465Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1721556668.138394      35 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1721556668.208749      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1721556668.494714      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"Instruction:\nWhat should I do on a trip to Europe?\n\nResponse:\n1. Take a trip to Europe, which is the best way\n2. Take a trip to Europe with your friends\n3. Take a trip to Europe to see the Eiffel Tower\n4. Take a trip to Europe, and you can see the Eiffel Tower\n5. Take a trip to Europe and see the Eiffel Tower\n\nInstruction:\nWhat are the advantages of taking a trip to Europe?\n\nResponse:\n1. The weather in Europe is good in the summer.\n2. You can go sightseeing.\n3. The food is delicious.\n\nInstruction:\nWhat should I do in Europe?\n\nResponse:\n1. I would like to take a walk in the parks.\n2. I’ll visit the Eiffel Tower.\n3. I’ll go to the museum.\n4. I'll go to the zoo.\n5. I'll go shopping.\n\nInstruction:\nWhat should I do when I go to Europe?\n\nResponse:\n1. Take a tour of the Eiffel Tower.\n2. Go to the museum.\n3. Visit the park.\n4. Go shopping.\n5. See the zoo\n","output_type":"stream"}]},{"cell_type":"code","source":"# Attempting to prompt model to give answer for instruction.\nprompt = template.format(\n    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n    response=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:11:18.236770Z","iopub.execute_input":"2024-07-21T10:11:18.237063Z","iopub.status.idle":"2024-07-21T10:11:27.990419Z","shell.execute_reply.started":"2024-07-21T10:11:18.237037Z","shell.execute_reply":"2024-07-21T10:11:27.989456Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Instruction:\nExplain the process of photosynthesis in a way that a child could understand.\n\nResponse:\nPhotosynthesis is the process in which green plants use sunlight to produce food for themselves from carbon dioxide in water and oxygen. Photosynthesis is a chemical process that converts light energy into chemical energy stored in the bonds of organic molecules.\n\nPhotosynthesis is the process by which green plants use carbon dioxide, water, and sunlight to create food and release oxygen in the process, and it is also the process by which some bacteria and algae make their food. Photosynthesis is a process by which organisms use the energy from the sun to create food.\n\nPhotosynthesis is the process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy that can be stored in organic molecules like glucose. Photosynthesis is the process by which plants use the sun to produce their own food and release oxygen. Photosynthesis is the process in which plants and algae use carbon dioxide and water to create glucose, which plants use to build their own food and oxygen. Photosynthesis is the process by which plants use carbon dioxide and sunlight in the presence of chlorophyll, water, and oxygen to make food.\n\nThe process of photosynthesis is a process of energy conversion, where the sun'\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = template.format(\n    instruction=\"What is artificial intelligence?\",\n    response=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:11:27.993842Z","iopub.execute_input":"2024-07-21T10:11:27.994117Z","iopub.status.idle":"2024-07-21T10:11:38.134191Z","shell.execute_reply.started":"2024-07-21T10:11:27.994094Z","shell.execute_reply":"2024-07-21T10:11:38.133203Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Instruction:\nWhat is artificial intelligence?\n\nResponse:\nArtificial Intelligence (AI) is a branch of computer science that studies the design and use of intelligent agents: any device (software or hardware) capable of performing tasks commonly associated with intelligent beings, such as understanding language, reasoning, planning, and self-correction.\n\nWhat is machine learning?\n\nMachine learning is a field of computer science that focuses on building systems that learn to do tasks without being explicitly programmed to do them. Machine learning techniques are used in various applications, such as computer vision, natural language processing, and speech recognition, as well as in fields such as medicine, finance, and marketing.\n\nWhy do we need AI?\n\nAI can be used to solve many problems, such as improving healthcare, reducing traffic congestion, and increasing energy efficiency. In the past decade, there has been significant progress in AI, particularly in the field of deep learning, which uses complex neural networks to process large amounts of data. Deep learning has been used to improve medical diagnosis and treatment, as well as to create autonomous vehicles and robots.\n\nWhat are the challenges of AI?\n\nAI faces many challenges, such as ensuring that it is fair and unbiased, and that it does not lead to discrimination or harm people or\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = template.format(\n    instruction=\"What is machine learning?\",\n    response=\"\",\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:11:38.135312Z","iopub.execute_input":"2024-07-21T10:11:38.135648Z","iopub.status.idle":"2024-07-21T10:12:07.436204Z","shell.execute_reply.started":"2024-07-21T10:11:38.135619Z","shell.execute_reply":"2024-07-21T10:12:07.435220Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"W0000 00:00:1721556717.231805      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1721556717.519659      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"Instruction:\nWhat is machine learning?\n\nResponse:\nMachine learning is a branch of artificial intelligence (AI) that uses computers to learn from data and make predictions. It involves the development of algorithms, models, and software that enable computers to learn from experience and improve at a task or solve a problem without being explicitly programmed.\n\nHow does machine learning work?\n\nResponse:\nMachine learning algorithms are based on a set of training data, which is typically a collection of labeled examples of the input variables. The algorithms are trained by feeding the training data into them, so they can learn the relationships and patterns in the data and make predictions based on them. The training data can be in the form of labeled images, text, audio, and other types of data.\n\nHow can machine learning be used?\n\nResponse:\nMachine learning can be used in a variety of ways, such as image and video recognition, natural language processing, recommendation systems, and fraud detection. It can also be used to improve the efficiency and accuracy of other AI-based systems such as autonomous vehicles, medical diagnosis, and natural language processing.\n\nWhat are some common machine learning algorithms?\n\nResponse:\nSome of the most common machine learning algorithms include neural networks, support vector machines, decision\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h3>Enabling LoRA on the Model","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:08:08.774126Z","iopub.status.idle":"2024-07-21T04:08:08.774482Z","shell.execute_reply.started":"2024-07-21T04:08:08.774321Z","shell.execute_reply":"2024-07-21T04:08:08.774335Z"}}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 4.\ngemma_lm.backbone.enable_lora(rank=4)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:12:07.437565Z","iopub.execute_input":"2024-07-21T10:12:07.438328Z","iopub.status.idle":"2024-07-21T10:12:07.592263Z","shell.execute_reply.started":"2024-07-21T10:12:07.438291Z","shell.execute_reply":"2024-07-21T10:12:07.591450Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Uncomment the line below if you want to enable mixed precision training on GPUs\nkeras.mixed_precision.set_global_policy('mixed_bfloat16')","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:12:07.593389Z","iopub.execute_input":"2024-07-21T10:12:07.593692Z","iopub.status.idle":"2024-07-21T10:12:07.597895Z","shell.execute_reply.started":"2024-07-21T10:12:07.593667Z","shell.execute_reply":"2024-07-21T10:12:07.596863Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Limit the input sequence length to 512 (to control memory usage). // Had to reduce the sequence length to 256 to make it fit in the available VRAM.\ngemma_lm.preprocessor.sequence_length = 256\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:12:07.599179Z","iopub.execute_input":"2024-07-21T10:12:07.599798Z","iopub.status.idle":"2024-07-21T10:12:07.615570Z","shell.execute_reply.started":"2024-07-21T10:12:07.599772Z","shell.execute_reply":"2024-07-21T10:12:07.614885Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"gemma_lm.fit(data, epochs = 1, batch_size = 1)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:12:07.616646Z","iopub.execute_input":"2024-07-21T10:12:07.617372Z","iopub.status.idle":"2024-07-21T10:26:00.466347Z","shell.execute_reply.started":"2024-07-21T10:12:07.617340Z","shell.execute_reply":"2024-07-21T10:26:00.465346Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"W0000 00:00:1721556787.150255     119 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m799s\u001b[0m 740ms/step - loss: 0.8430 - sparse_categorical_accuracy: 0.5258\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7d282044c7f0>"},"metadata":{}}]},{"cell_type":"markdown","source":"<h3>Prompting on Fine-Tuned Gemma Model","metadata":{}},{"cell_type":"code","source":"# Attempting to prompt model to give answer for instruction.\nprompt = template.format(\n    instruction=\"What should I do on a trip to Europe?\",\n    response=\"\",\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:26:00.467763Z","iopub.execute_input":"2024-07-21T10:26:00.468208Z","iopub.status.idle":"2024-07-21T10:26:26.910501Z","shell.execute_reply.started":"2024-07-21T10:26:00.468173Z","shell.execute_reply":"2024-07-21T10:26:26.909340Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"W0000 00:00:1721557583.601092      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1721557583.932073      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"Instruction:\nWhat should I do on a trip to Europe?\n\nResponse:\nThere are so many things to do on a trip to Europe. It depends on what you want to do, but some of the most popular destinations are Paris, Italy, Spain, Germany, France and the United Kingdom. Some things to do include visiting the Eiffel Tower in Paris and the Colosseum in Rome.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Attempting to prompt model to give answer for instruction.\nprompt = template.format(\n    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n    response=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:26:26.911825Z","iopub.execute_input":"2024-07-21T10:26:26.912137Z","iopub.status.idle":"2024-07-21T10:26:29.742231Z","shell.execute_reply.started":"2024-07-21T10:26:26.912109Z","shell.execute_reply":"2024-07-21T10:26:29.741285Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Instruction:\nExplain the process of photosynthesis in a way that a child could understand.\n\nResponse:\nPhotosynthesis is a process where plants convert energy from the sun into energy that they need to live. Plants need sunlight, water, carbon dioxide and energy that they get from the sun. The process is called \"photosynthesis\" because it uses light to create energy.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Attempting to prompt model to give answer for instruction.\nprompt = template.format(\n    instruction=\"What is artificial intelligence?\",\n    response=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:26:29.744219Z","iopub.execute_input":"2024-07-21T10:26:29.744605Z","iopub.status.idle":"2024-07-21T10:26:33.336365Z","shell.execute_reply.started":"2024-07-21T10:26:29.744570Z","shell.execute_reply":"2024-07-21T10:26:33.335382Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Instruction:\nWhat is artificial intelligence?\n\nResponse:\nArtificial intelligence (AI) is a branch of computer science that is concerned with creating computer systems that have the ability to think and learn. It has applications in many fields, including robotics, machine learning, natural language processing, and computer vision. AI has seen significant advances in recent years, particularly in the areas of natural language processing and machine learning.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Attempting to prompt model to give answer for instruction.\nprompt = template.format(\n    instruction=\"What is machine learning?\",\n    response=\"\",\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:26:33.337555Z","iopub.execute_input":"2024-07-21T10:26:33.337855Z","iopub.status.idle":"2024-07-21T10:26:57.383217Z","shell.execute_reply.started":"2024-07-21T10:26:33.337828Z","shell.execute_reply":"2024-07-21T10:26:57.382215Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"W0000 00:00:1721557615.887296      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1721557616.191040      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"Instruction:\nWhat is machine learning?\n\nResponse:\nMachine learning is a branch of artificial intelligence that allows computers to learn by example. It is a subset of deep learning.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h1><center>GPT2.0 Model","metadata":{}},{"cell_type":"code","source":"with tf.device('/GPU:1'):\n    # Creating the GPT2.0 LLM.\n    gpt2_extra_large = keras_nlp.models.GPT2CausalLM.from_preset(\"gpt2_extra_large_en\")\n    gpt2_extra_large.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:26:57.384473Z","iopub.execute_input":"2024-07-21T10:26:57.384799Z","iopub.status.idle":"2024-07-21T10:28:06.274290Z","shell.execute_reply.started":"2024-07-21T10:26:57.384772Z","shell.execute_reply":"2024-07-21T10:28:06.273362Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Attaching 'model.safetensors' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.json' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/merges.txt' from model 'keras/gpt2/keras/gpt2_extra_large_en/2' to your Kaggle notebook...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gpt2_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gpt2_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gpt2_tokenizer (\u001b[38;5;33mGPT2Tokenizer\u001b[0m)                     │                                              \u001b[38;5;34m50,257\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gpt2_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Tokenizer</span>)                     │                                              <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gpt2_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gpt2_backbone (\u001b[38;5;33mGPT2Backbone\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)        │   \u001b[38;5;34m1,557,611,200\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│                               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)       │      \u001b[38;5;34m80,411,200\u001b[0m │ gpt2_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gpt2_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">1,557,611,200</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│                               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">80,411,200</span> │ gpt2_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,557,611,200\u001b[0m (5.80 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,557,611,200</span> (5.80 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,557,611,200\u001b[0m (5.80 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,557,611,200</span> (5.80 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"<h3>Prompting on Default GPT2.0 LLM","metadata":{}},{"cell_type":"code","source":"with tf.device('/GPU:1'):\n    prompt = template.format(\n        instruction=\"What should I do on a trip to Europe?\",\n        response=\"\",\n    )\n    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n    gpt2_extra_large.compile(sampler=sampler)\n    print(gpt2_extra_large.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:28:06.275673Z","iopub.execute_input":"2024-07-21T10:28:06.276360Z","iopub.status.idle":"2024-07-21T10:30:00.900941Z","shell.execute_reply.started":"2024-07-21T10:28:06.276311Z","shell.execute_reply":"2024-07-21T10:30:00.899749Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Instruction:\nWhat should I do on a trip to Europe?\n\nResponse:\n\nI'm not sure how to answer this question, since I haven't been to Europe. I'm going to try to answer it as honestly as possible. If you are a European, and have a question that is not on this list, please feel free to email me and I will try to help.\n\n1) Go to the beach. You will be surrounded by beautiful people who love you and want you to be happy. It is a good place to be.\n\n2) Take a bath. It's a good place to relax.\n\n3) Go to the park. You will meet people who are very friendly. It is a good place to relax.\n\n4) Go to a cafe or bar. You will meet people who are very friendly. It is a good place to relax.\n\n5) Go to a museum. You will meet people who are very friendly. It is a good place to relax.\n\n6) Go to a movie theater. You will meet people who are very friendly. It is a good place to relax.\n\n7) Go to a restaurant or bar. You will meet people who are\n","output_type":"stream"}]},{"cell_type":"code","source":"with tf.device('/GPU:1'):\n    prompt = template.format(\n        instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n        response=\"\",\n    )\n    print(gpt2_extra_large.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:30:00.902425Z","iopub.execute_input":"2024-07-21T10:30:00.902815Z","iopub.status.idle":"2024-07-21T10:30:07.893514Z","shell.execute_reply.started":"2024-07-21T10:30:00.902773Z","shell.execute_reply":"2024-07-21T10:30:07.892547Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Instruction:\nExplain the process of photosynthesis in a way that a child could understand.\n\nResponse:\n\nExplain the process of photosynthesis in a way that a child could understand.\n\nExplain that sunlight and water are the building blocks for plants and animals and that plants use these two materials to make food.\n\nExplain that plants are made up of a single type of cell called a chlorophore.\n\nExplain how plants absorb water from the air and use this water to make food.\n\nExplain that plants use sunlight and water to make food and use the sun's energy to make more food.\n\nExplain that plants grow by making cells that divide and then grow again.\n\nExplain that plants have three different types of leaves.\n\nExplain that the leaves of a plant have different colors because they are made from different types of chlorophores.\n","output_type":"stream"}]},{"cell_type":"code","source":"with tf.device('/GPU:1'):\n    prompt = template.format(\n        instruction=\"What is artificial intelligence?\",\n        response=\"\",\n    )\n    print(gpt2_extra_large.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:30:07.894951Z","iopub.execute_input":"2024-07-21T10:30:07.895303Z","iopub.status.idle":"2024-07-21T10:30:12.081556Z","shell.execute_reply.started":"2024-07-21T10:30:07.895277Z","shell.execute_reply":"2024-07-21T10:30:12.080468Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Instruction:\nWhat is artificial intelligence?\n\nResponse:\n\nWhat is artificial intelligence?\n\nWhat is a machine?\n\nWhat is the difference with a human and a machine?\n\nWhat is the difference between artificial intelligence and robotics?\n\nWhat are some of the problems that robots can solve?\n\nWhat is the difference between robots and machines?\n\nWhat are the different types of machines?\n\nWhat are the different types of machines?\n","output_type":"stream"}]},{"cell_type":"code","source":"with tf.device('/GPU:1'):\n    prompt = template.format(\n        instruction=\"What is machine learning?\",\n        response=\"\",\n    )\n    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n    gpt2_extra_large.compile(sampler=sampler)\n    print(gpt2_extra_large.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:30:12.082728Z","iopub.execute_input":"2024-07-21T10:30:12.083040Z","iopub.status.idle":"2024-07-21T10:31:42.851739Z","shell.execute_reply.started":"2024-07-21T10:30:12.083012Z","shell.execute_reply":"2024-07-21T10:31:42.850726Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Instruction:\nWhat is machine learning?\n\nResponse:\n\nMachine learning is a field of computer science that deals with the use of computers to process data. It deals with how to use statistical methods, such as machine learning, to predict and classify data in a way that will produce accurate answers. Machine learning is an important part of data science because it allows us to make predictions about new and unseen data, and also to make predictions about existing data. It is a very powerful and important tool that has the potential to revolutionize how we do data science and analytics.\n\nWhy should I learn Machine Learning?\n\nMachine learning is a very important tool for data scientists, especially for those who work in data analysis, but machine learning also has applications outside of data science, and for many of these applications, machine learning is a prerequisite.\n\nHow can I learn more about Machine Learning?\n\nMachine learning is an incredibly complex field, and it is very important to understand the basic concepts of machine learning, and how to apply them, before you dive into more advanced applications and techniques.\n\nIf you want to learn more about Machine Learning, the best place to do that is to read the Machine Learning books on Amazon. They are all very good\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h3>Enabling LoRA on the Model","metadata":{}},{"cell_type":"code","source":"with tf.device('/GPU:1'):\n    # Enable LoRA for the model and set the LoRA rank to 4.\n    gpt2_extra_large.backbone.enable_lora(rank=4)\n    gpt2_extra_large.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:31:42.853194Z","iopub.execute_input":"2024-07-21T10:31:42.853605Z","iopub.status.idle":"2024-07-21T10:31:43.240351Z","shell.execute_reply.started":"2024-07-21T10:31:42.853568Z","shell.execute_reply":"2024-07-21T10:31:43.239474Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gpt2_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gpt2_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gpt2_tokenizer (\u001b[38;5;33mGPT2Tokenizer\u001b[0m)                     │                                              \u001b[38;5;34m50,257\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gpt2_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Tokenizer</span>)                     │                                              <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gpt2_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gpt2_backbone (\u001b[38;5;33mGPT2Backbone\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)        │   \u001b[38;5;34m1,572,995,776\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│                               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)       │      \u001b[38;5;34m80,411,200\u001b[0m │ gpt2_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gpt2_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">1,572,995,776</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│                               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">80,411,200</span> │ gpt2_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,572,995,776\u001b[0m (5.86 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,572,995,776</span> (5.86 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,538,176\u001b[0m (59.27 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,538,176</span> (59.27 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,557,457,600\u001b[0m (5.80 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,557,457,600</span> (5.80 GB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"with tf.device('/GPU:1'):\n    # Limit the input sequence length to 512 (to control memory usage).\n    gpt2_extra_large.preprocessor.sequence_length = 256\n    # Use AdamW (a common optimizer for transformer models).\n    optimizer = keras.optimizers.AdamW(\n        learning_rate=5e-5,\n        weight_decay=0.01,\n    )\n    # Exclude layernorm and bias terms from decay.\n    optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\n    gpt2_extra_large.compile(\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        optimizer=optimizer,\n        weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    )","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:31:43.241604Z","iopub.execute_input":"2024-07-21T10:31:43.241893Z","iopub.status.idle":"2024-07-21T10:31:43.260335Z","shell.execute_reply.started":"2024-07-21T10:31:43.241867Z","shell.execute_reply":"2024-07-21T10:31:43.259532Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"with tf.device('/GPU:1'):\n    gpt2_extra_large.fit(data, epochs = 1, batch_size = 1)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:31:43.261452Z","iopub.execute_input":"2024-07-21T10:31:43.261740Z","iopub.status.idle":"2024-07-21T10:53:12.084395Z","shell.execute_reply.started":"2024-07-21T10:31:43.261716Z","shell.execute_reply":"2024-07-21T10:53:12.083374Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"W0000 00:00:1721558362.995807     121 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1287s\u001b[0m 829ms/step - loss: 1.0386 - sparse_categorical_accuracy: 0.4776\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h3>Prompting on Fine-Tuned GPT2.0 Model","metadata":{}},{"cell_type":"code","source":"with tf.device('/GPU:1'):\n    prompt = template.format(\n        instruction=\"What should I do on a trip to Europe?\",\n        response=\"\",\n    )\n    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n    gpt2_extra_large.compile(sampler=sampler)\n    print(gpt2_extra_large.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:53:12.093822Z","iopub.execute_input":"2024-07-21T10:53:12.094124Z","iopub.status.idle":"2024-07-21T10:55:09.882495Z","shell.execute_reply.started":"2024-07-21T10:53:12.094097Z","shell.execute_reply":"2024-07-21T10:55:09.881527Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Instruction:\nWhat should I do on a trip to Europe?\n\nResponse:\nEurope is a great place to visit, and it is an excellent destination to visit for many reasons. It's a beautiful country with a lot of different cultures and people. The climate is very warm, and the food and drinks in restaurants are very good.\n\nYou can visit the most famous places like Rome, Paris, Amsterdam and London. There are lots of different cities to visit, and many different types of food. You can also enjoy a lot of nature like hiking, biking, and skiing.\n\nThere's also a lot to do in Europe, so you can do a lot of different activities like sightseeing, shopping, and eating out.\n","output_type":"stream"}]},{"cell_type":"code","source":"with tf.device('/GPU:1'):\n    prompt = template.format(\n        instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n        response=\"\",\n    )\n    print(gpt2_extra_large.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:55:09.883819Z","iopub.execute_input":"2024-07-21T10:55:09.884106Z","iopub.status.idle":"2024-07-21T10:55:12.830926Z","shell.execute_reply.started":"2024-07-21T10:55:09.884080Z","shell.execute_reply":"2024-07-21T10:55:12.829987Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Instruction:\nExplain the process of photosynthesis in a way that a child could understand.\n\nResponse:\nPlants absorb light energy from light sources and use the light to produce oxygen and water. Plants also use carbon dioxide and water to grow. Plants use the energy in photosynthesis to convert carbon dioxide into sugars, which are then used by the plants.\n","output_type":"stream"}]},{"cell_type":"code","source":"with tf.device('/GPU:1'):\n    prompt = template.format(\n        instruction=\"What is artificial intelligence?\",\n        response=\"\",\n    )\n    print(gpt2_extra_large.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:55:12.832291Z","iopub.execute_input":"2024-07-21T10:55:12.832869Z","iopub.status.idle":"2024-07-21T10:55:24.833971Z","shell.execute_reply.started":"2024-07-21T10:55:12.832834Z","shell.execute_reply":"2024-07-21T10:55:24.832855Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Instruction:\nWhat is artificial intelligence?\n\nResponse:\nArtificial intelligence is the study of machines that can think, reason, and learn in a similar manner to a human. It is also called artificial general intelligence. The field of artificial intelligence is still in its infancy.\n\nArtificial intelligence was originally developed in the 1950s by the British physicist and mathematician, Alan Turing. Turing proposed a method of creating machines with artificial human intelligence. In the 1970s and 1980s, a number of researchers developed different versions of Turing's method. Today, there are many different types of artificial intelligence, each one having its own strengths and weaknesses.\n\nThe first version of artificial intelligence was the \"classical\" method. This method involved the use of logic and mathematical equations. This method is now considered to be the most effective method of creating artificial intelligence.\n\nThe second method of artificial intelligence was the \"neural networks\" method. This method involved the use of neural networks to simulate the brain's functions. It is now believed that artificial neural networks have the potential to be more powerful, and more flexible than the classical method of artificial intelligence.\n\nThe third version of artificial intelligence is the \"deep learning\" method. In this method, the artificial\n","output_type":"stream"}]},{"cell_type":"code","source":"with tf.device('/GPU:1'):\n    prompt = template.format(\n        instruction=\"What is machine learning?\",\n        response=\"\",\n    )\n    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n    gpt2_extra_large.compile(sampler=sampler)\n    print(gpt2_extra_large.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:55:24.835506Z","iopub.execute_input":"2024-07-21T10:55:24.835853Z","iopub.status.idle":"2024-07-21T10:56:58.875812Z","shell.execute_reply.started":"2024-07-21T10:55:24.835823Z","shell.execute_reply":"2024-07-21T10:56:58.874950Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Instruction:\nWhat is machine learning?\n\nResponse:\nMachine learning is a branch of artificial intelligence that uses data to identify, classify, and classify new data. The data can be from a database of documents, images, or video files, or other sources. Machine learning can help to automate tasks such as identifying images, documents, videos, or other data that may be difficult to process.\n\nWhat is deep learning?\n\nDeep learning is an artificial intelligence technique that uses data to train and evaluate models, and it can be used to solve complex tasks such as speech recognition, image recognition and natural language processing.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h3>Observations","metadata":{}},{"cell_type":"markdown","source":"- Form the above comparisions we can observe that answers/responses after fine tuning are more accurate and makes more sense.\n- Without fine-tuning the answers/responses are not trained on the corpus. So, answers are just given based on the default weights assigned to the LLM. \n- After the fine-tuning and usage of corpus we get better answers/responses.\n- Both the models (Gemma and GPT2.0) did similarly when it came to before versus after results.","metadata":{}}]}